{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter corpus creation and LDA topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has two sections:\n",
    "1. Prototype code for creating a gensim-compatible corpus from my collection of tweets.\n",
    "2. Training an LDA topic model on a subset of the corpus.\n",
    "\n",
    "This is largely prototyping and experimenting with model hyperparameters. When done, I'll create separate scripts for each part of this. There's a very rudimentary version of this (probably not committed. Oops...) using a Wikipedia corpus for training, but I wasn't satisfied with it and retraining takes about 12 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python libs\n",
    "import sys, os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "# Database\n",
    "import pymongo\n",
    "\n",
    "# NLP libs\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import TfidfModel, CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "\n",
    "# Visualisation libs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From src/data/db_handlers.py\n",
    "def mongodb_connect():\n",
    "    \"\"\"\n",
    "    Establish connection to MongoDB\n",
    "    db name given in .env file\n",
    "    \"\"\"\n",
    "    dotenv_path = find_dotenv()\n",
    "    load_dotenv(dotenv_path)\n",
    "\n",
    "    client = pymongo.MongoClient(os.environ.get(\"DATABASE_URL\"), 27017)\n",
    "    db = client.tweetbase\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = mongodb_connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tweetbase db contains two separate collections of tweets, one from Aberdeen, Scotland and the second from Hammersmith, London."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"There are {db.tweets_abdn.count()} tweets from Aberdeen \n",
    "and {db.tweets_hsmith.count()} tweets from Hammersmith in the db\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get full_text field for each tweet\n",
    "# Only the first 50 tweets were returned for testing purposes\n",
    "tweets_abdn = db.tweets_abdn.find({}, {\"_id\": 0, \"full_text\": 1})[:50000]\n",
    "tweets_hsmith = db.tweets_hsmith.find({}, {\"_id\": 0, \"full_text\": 1})[:50000]\n",
    "\n",
    "# Convert mongodb cursor objects into lists\n",
    "tweets_abdn = [_.get(\"full_text\") for _ in tweets_abdn]\n",
    "tweets_hsmith = [_.get(\"full_text\") for _ in tweets_hsmith]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets_abdn + tweets_hsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get indices for a random sample of 50 tweets for inspection\n",
    "tweets_sample_idxs = [idx for idx, _ in random.sample(list(enumerate(tweets)), 50)]\n",
    "pprint([tweets[i] for i in tweets_sample_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove links and @ prefixes from tweets\n",
    "tweets = [re.sub('@|https?\\://\\S+', '', t) for t in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint([tweets[i] for i in tweets_sample_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization of the tweets was performed with `gensim.utils.simple_preprocess()`. This method only produces unigram tokens. Using `gensim.models.phrases.Phraser` on the tokenized output should create bigrams, but it is unclear at present whether the method used below actually did for the given input..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_tokens = [simple_preprocess(t) for t in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = Text8Corpus(datapath('testcorpus.txt'))\n",
    "phrases = Phrases(sentences, min_count=1, threshold=1)\n",
    "\n",
    "# bigram = Phrases(common_texts)\n",
    "bigram = Phraser(phrases)\n",
    "tweets_tokens = [bigram[t] for t in tweets_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint([tweets_tokens[i] for i in tweets_sample_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "print(stop)\n",
    "whitelist = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason negated forms of *should*, *would* and *might* are included, but not the regular forms. I've added them to the list myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_additions = ['should', 'would', 'might', 'could']\n",
    "stop = stop + stop_additions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitelist = []\n",
    "\n",
    "tweets_tokens = [[word for word in sentence if word in whitelist or word not in stop]\n",
    "     for sentence in tweets_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint([tweets_tokens[i] for i in tweets_sample_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is grouping words under their lemma, or dictionary form e.g. *knows* and *knew* under *know*, or *feet* under *foot*. This requires knowledge of the Part of Speech (PoS) of the item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While lemmatization was performed on the corpus, it should be noted that it may not necessarily be beneficial. [Schofield and Mimno (2016)](https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00099) report that, at best, text preprocessed with the NLTK WordNet lemmatizer showed no meaningful change in topic coherence scores when it comes to LDA topic modelling compared with data that had not been stemmed. At worst, some stemming methods decrease LDA topic stability.\n",
    "\n",
    "TODO: Possibly compare a lemmatized and unlemmatized version of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "tweets_tokens = [[wnl.lemmatize(word, get_wordnet_pos(word)) for word in sentence]\n",
    "    for sentence in tweets_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint([tweets_tokens[i] for i in tweets_sample_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_tmp = set()\n",
    "for sentence in tweets_tokens:\n",
    "    for word in sentence:\n",
    "        set_tmp.add(word)\n",
    "print(f\"There are {len(set_tmp)} unique words in the corpus\")\n",
    "print(f\"The first 150 words are:\\n {list(set_tmp)[:150]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the words in the corpus so far are Twitter username mentions and hashtags. The creation of the dictionary in the next section will filter those that are not widely used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dictionary and BoW corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_WORDS = 100000 # Max number of words in dictionary\n",
    "CORPUS_PATH = Path('../../data/corpora') # Location to save corpus and dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(tweets_tokens)\n",
    "\n",
    "# Filter the dictionary:\n",
    "#    Words must appear in no fewer than n documents\n",
    "#    Words must not appear in more than n of the total documents\n",
    "dictionary.filter_extremes(no_below=40, no_above=0.05, keep_n=KEEP_WORDS)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with the LDA model hyperparameters, the dictionary filtering above will have a considerable impact on the topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the corpus\n",
    "tweet_corpus = [dictionary.doc2bow(tweet) for tweet in tweets_tokens] # Use the dict to create BoW vectors\n",
    "# tweet_corpus = [text for text in tweet_corpus if len(text) > 0]\n",
    "MmCorpus.serialize(str(CORPUS_PATH / 'tweets') + '_bow.mm', tweet_corpus, progress_cnt=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save_as_text(str(CORPUS_PATH / 'tweets') + '_wordids.txt.bz2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
